{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-site latent localized regression on Prevent Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Load own module for data loading and training\n",
    "import sys\n",
    "sys.path.append('../../src/methods')\n",
    "from single_site_ae_loreg_module import create_data_loader, initialize_training, train_model\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)  # If using CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevent = pd.read_csv('../../data/processed/prevent_num_imp_varL2_std.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_site_data = prevent\n",
    "\n",
    "# Drop unnecessary columns\n",
    "single_site_data = single_site_data.drop(columns=['SGRQ_symptoms_W', \n",
    "                                                  'SGRQ_activity_W', \n",
    "                                                  'SGRQ_impacts_W', \n",
    "                                                  'CAT_Score_W'])\n",
    "\n",
    "# Get SGRQ_total_W from respective next visit to predict based on previous visit\n",
    "single_site_data['SGRQ_total_shift'] = single_site_data.groupby(['PID'])['SGRQ_total_W'].shift(-1)\n",
    "\n",
    "# Remove original `SGRQ_total_W` column\n",
    "single_site_data = single_site_data.drop(columns=[\"SGRQ_total_W\"])\n",
    "\n",
    "# Remove NAs created in `SGRQ_total_shift`\n",
    "single_site_data = single_site_data.dropna()\n",
    "\n",
    "# Filter vor only Baseline visit\n",
    "single_site_data = single_site_data[single_site_data['VISIT']==\"Baseline (V0)\"]\n",
    "\n",
    "# Drop unneeded columns: `CENTER`, `PID`, `VISIT`\n",
    "single_site_data = single_site_data.drop(columns=['CENTER', 'PID', 'VISIT'])\n",
    "\n",
    "# Remove outliers\n",
    "Q1 = single_site_data.quantile(0.25)\n",
    "Q3 = single_site_data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "single_site_data = single_site_data[~((single_site_data < (Q1 - 4 * IQR)) | (single_site_data > (Q3 + 4 * IQR))).any(axis=1)]\n",
    "\n",
    "# Reset index\n",
    "single_site_data = single_site_data.reset_index(drop=True)\n",
    "\n",
    "# Save processed data\n",
    "# single_site_data.to_csv('../../data/processed/prevent_num_imp_varL2_std_single_site.csv', index=False)\n",
    "\n",
    "# Print size of the single-site data\n",
    "single_site_size = len(single_site_data)\n",
    "print(single_site_size)\n",
    "\n",
    "# Normalize each column\n",
    "single_site_data = (single_site_data - single_site_data.mean()) / single_site_data.std()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split data into train and test sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(single_site_data, test_size=0.2, random_state=123)\n",
    "\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "test_data_later = test_data.copy()\n",
    "\n",
    "print(len(train_data), len(test_data))\n",
    "\n",
    "# Save processed data\n",
    "single_site_data.to_csv('../../data/processed/prevent_num_imp_varL2_std_single_site.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters and settings\n",
    "input_size = single_site_data.shape[1] - 1\n",
    "latent_size = 4\n",
    "hidden_size_1 = latent_size**3\n",
    "hidden_size_2 = latent_size**2\n",
    "target_size = single_site_data.shape[1] - 1\n",
    "batch_size = 1\n",
    "num_epochs = 200\n",
    "log_interval = 2\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-4\n",
    "alpha_values = [1]\n",
    "theta_values = [0.021]\n",
    "gamma_values = [1e-4]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "kernel = 'gaussian'\n",
    "num_batches = 3\n",
    "sigma = 1.2\n",
    "k_nearest = 0.3\n",
    "\n",
    "# Main training loop\n",
    "tune_iter = 1\n",
    "loss_values = []\n",
    "\n",
    "\n",
    "for alpha, theta, gamma in itertools.product(alpha_values, theta_values, gamma_values):\n",
    "    # dynamic log dir for multiple runs\n",
    "    log_dir = f\"../../runs/alpha{alpha}_theta{theta}_gamma{gamma}_sigma{sigma}_weightdecay{weight_decay}_k_nearest{k_nearest}_batch{num_batches}_latent{latent_size}_epochs{num_epochs}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir) \n",
    "\n",
    "    print(f\"Tuning parameter combination {tune_iter} with alpha={alpha}, theta={theta}, gamma={gamma}\")\n",
    "    tune_iter += 1\n",
    "\n",
    "    # Initialize model and optimizer\n",
    "    model, optimizer, loss_values = initialize_training(input_size, \n",
    "                                                        hidden_size_1, \n",
    "                                                        hidden_size_2, \n",
    "                                                        latent_size, \n",
    "                                                        device, \n",
    "                                                        learning_rate, \n",
    "                                                        weight_decay)\n",
    "\n",
    "    # Train the model\n",
    "    current_loss_values = train_model(train_data,\"SGRQ_total_shift\",batch_size, 1000, num_batches, writer, model, optimizer, num_epochs, log_interval, alpha, theta, gamma, device, sigma, k_nearest, kernel)\n",
    "    loss_values.append(current_loss_values)\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save away the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_loader = create_data_loader(train_data, \"SGRQ_total_shift\", batch_size, num_batches, shuffle=False)\n",
    "latent_data = model.encoder(data_loader.dataset.data_x).detach().numpy()\n",
    "\n",
    "# Reconstruction training\n",
    "reconstruction_train = model.decoder(torch.tensor(latent_data).float()).detach().numpy()\n",
    "reconstruction_train = pd.DataFrame(reconstruction_train, columns=train_data.columns[:-1])\n",
    "\n",
    "# Reconstruction loss\n",
    "reconstruction_loss_train = np.mean((train_data - reconstruction_train)**2)\n",
    "print(f\"Reconstruction loss: {reconstruction_loss_train}\")\n",
    "\n",
    "latent_data = pd.DataFrame(latent_data, columns=[f'Latent{i}' for i in range(latent_size)])\n",
    "\n",
    "# Merge with y column\n",
    "latent_data['Y'] = train_data[\"SGRQ_total_shift\"]\n",
    "latent_data.head()\n",
    "\n",
    "print(len(data_loader.dataset.data_x), len(latent_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_latent_spaces(latent_df, latent_size):\n",
    "    # Calculate the number of rows and columns for subplots\n",
    "    n_cols = 2\n",
    "    n_rows = (latent_size + 1) // n_cols\n",
    "\n",
    "    # Create a figure and set of subplots\n",
    "    plt.figure(figsize=(n_cols * 5, n_rows * 5))\n",
    "    \n",
    "    for i in range(latent_size):\n",
    "        plt.subplot(n_rows, n_cols, i + 1)\n",
    "        sns.scatterplot(data=latent_df, x=f'Latent{i}', y='Y', alpha=0.5)\n",
    "        plt.title(f'Latent Space {i} vs Y of AE with Regression')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot latent spaces\n",
    "plot_latent_spaces(latent_data, latent_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_site_data = prevent\n",
    "\n",
    "# Drop unnecessary columns\n",
    "single_site_data = single_site_data.drop(columns=['SGRQ_symptoms_W', \n",
    "                                                  'SGRQ_activity_W', \n",
    "                                                  'SGRQ_impacts_W', \n",
    "                                                  'CAT_Score_W'])\n",
    "\n",
    "# Get SGRQ_total_W from respective next visit to predict based on previous visit\n",
    "single_site_data['SGRQ_total_shift'] = single_site_data.groupby(['PID'])['SGRQ_total_W'].shift(-1)\n",
    "\n",
    "# Remove original `SGRQ_total_W` column\n",
    "single_site_data = single_site_data.drop(columns=[\"SGRQ_total_W\"])\n",
    "\n",
    "# Remove NAs created in `SGRQ_total_shift`\n",
    "single_site_data = single_site_data.dropna()\n",
    "\n",
    "# Filter vor only Baseline visit\n",
    "# single_site_data = single_site_data[single_site_data['VISIT'].isin(['Baseline (V0)', 'Visit 5 (SV5)'])]\n",
    "single_site_data = single_site_data[single_site_data['VISIT']==\"Baseline (V0)\"]\n",
    "\n",
    "# Drop unneeded columns: `CENTER`, `PID`, `VISIT`\n",
    "single_site_data = single_site_data.drop(columns=['CENTER', 'PID', 'VISIT'])\n",
    "\n",
    "# Remove outliers\n",
    "Q1 = single_site_data.quantile(0.25)\n",
    "Q3 = single_site_data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "single_site_data = single_site_data[~((single_site_data < (Q1 - 4 * IQR)) | (single_site_data > (Q3 + 4 * IQR))).any(axis=1)]\n",
    "\n",
    "# Normalize each column\n",
    "single_site_data = (single_site_data - single_site_data.mean()) / single_site_data.std()\n",
    "\n",
    "# Reset index\n",
    "single_site_data = single_site_data.reset_index(drop=True)\n",
    "\n",
    "train_data, test_data = train_test_split(single_site_data, test_size=0.2, random_state=123)\n",
    "\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "# Merge with latent data\n",
    "train_latent = pd.concat([train_data, latent_data], axis=1)\n",
    "\n",
    "# Drop outcome\n",
    "train_latent['Y'] = train_latent['SGRQ_total_shift']\n",
    "train_latent = train_latent.drop(columns=['SGRQ_total_shift'])\n",
    "\n",
    "print(len(train_data), len(train_latent))\n",
    "\n",
    "# Save processed data\n",
    "# train_latent.to_csv(f'../../results/tables/singlesite/train_merged_latent_alpha{alpha}_theta{theta}_gamma{gamma}_sigma{sigma}_weightdecay{weight_decay}_k_nearest{k_nearest}_batch{num_batches}_latent{latent_size}_epochs{num_epochs}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = create_data_loader(test_data, \"SGRQ_total_shift\", batch_size, num_batches, shuffle=False)\n",
    "latent_data = model.encoder(data_loader.dataset.data_x).detach().numpy()\n",
    "\n",
    "reconstructed_test = model.decoder(torch.tensor(latent_data).float()).detach().numpy()\n",
    "reconstructed_test = pd.DataFrame(reconstructed_test, columns=test_data.columns[:-1])\n",
    "\n",
    "# Reconstruction loss\n",
    "reconstruction_loss = np.mean((test_data - reconstructed_test)**2)\n",
    "print(f\"Reconstruction loss: {reconstruction_loss}\")\n",
    "\n",
    "latent_data = pd.DataFrame(latent_data, columns=[f'Latent{i}' for i in range(latent_size)])\n",
    "\n",
    "# Merge with y column\n",
    "latent_data['Y'] = test_data[\"SGRQ_total_shift\"]\n",
    "latent_data.head()\n",
    "\n",
    "test_latent = pd.concat([test_data, latent_data], axis=1)\n",
    "test_latent = test_latent.drop(columns=['SGRQ_total_shift'])\n",
    "\n",
    "print(len(test_latent))\n",
    "print(len(train_latent))\n",
    "\n",
    "# Merge train and test data\n",
    "train_latent['train'] = 1\n",
    "test_latent['train'] = 0\n",
    "\n",
    "# Row bind train and test data\n",
    "merged_latent = pd.concat([train_latent, test_latent], axis=0)\n",
    "\n",
    "merged_latent.to_csv(f'../../results/tables/singlesite/traintest_latent_alpha{alpha}_theta{theta}_gamma{gamma}_sigma{sigma}_weightdecay{weight_decay}_k_nearest{k_nearest}_batch{num_batches}_latent{latent_size}_epochs{num_epochs}.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
